\documentclass[a4paper]{article}

\usepackage{amsmath, amssymb}

\pdfsuppresswarningpagegroup=1

\begin{document}

\section{Vector: 3 perspectives - Physics, Math, CS }

\begin{align*}
	\begin{bmatrix}
		7 & 8 & 6 \\
		6 & 8 & 4 \\
		7 & 5 & 4 \\
	\end{bmatrix}
\end{align*}

\subsection{physics}
\begin{itemize}
	\item vectors are arrows in space with length and direction
	\item so vectors can be moved around in space without any issues
\end{itemize}

\subsection{cs}
\begin{itemize}
	\item vectors are ordered lists of numbers
\end{itemize}

\subsection{math}
\begin{itemize}
	\item seeks to generalize both of these views and defines vector operations such as
	      addition and multiplication
\end{itemize}

\subsection{In linear algebra}
\begin{itemize}
	\item graham suggests to view vector as arrow with tail always fixed at origin
	\item maybe i can imagine a vector as a operation of shifting the origin
	\item thus when we do vector addition we will start the second vector from the
	      start of first vector, since the origin has been shifted by the first vector
	\item Note: "but" it could really be shifting of all the points in the coordinate system
	      (as told by graham)
\end{itemize}

\subsection{Scalars}
\begin{itemize}
	\item The numbers that we multiply the vector with to scale the vector
	      in its original direction
	\item since it's used frequently it just interchangeable with number
\end{itemize}


\newpage
\section{Linear Combinations, Spans, Basis Vectors}

\subsection{Vector Coordinates}
\begin{itemize}
	\item vector coordinates are the number numbers present in a vector
	\item Each of the vector coordinates is also a scalar
	      that scales the basis vectors of the coordinate system $\hat{i}, \hat{j}$
\end{itemize}

\subsection{Span}
Span of $\vec{v} \text{ and } \vec{w}$ are the set of all of their linear combinations
\[
	a \vec{v} + b \vec{w}
\]

Note: Its common to think a collection of vectors as points, due to clustering/noise

\subsection{Linearly dependent Vectors}
\begin{itemize}
	\item If the third vector can be formed by linear combination of the other vectors
	      then the vectors are said to be linearly dependent
	\item If we cannot get a vector by linear combination of other vectors then
	      then those vectors are called as linearly independent
\end{itemize}

\subsection{Basis}
\begin{itemize}
	\item basis of a vector space is a set of linearly independent vectors
	      that span the full space
\end{itemize}

\newpage
\section{Linear Transformation and matrices}
Transformations are just "functions" just suggests to visualize
every function as a movement of coordinates(changing of basis vectors,
thus changing of all of coordinate system)

\subsection{Linear transformation in graph}
\begin{itemize}
	\item every lines will remain parallel
	\item and origin will remain at center
\end{itemize}

\subsection{Matrix Transformation}
\begin{itemize}
	\item A 2 dimensional linear transformation of a coordinates
	      system can be entirely described just 4 numbers (a 2d matrix)
	\item  In a matrix each of the column is the final landing
	      basis vector
	\item thus, matrix $\implies$ transformation data
	\item \[
		      \begin{bmatrix} a & b \\
                c & d\end{bmatrix}
		      \begin{bmatrix} x \\ y
		      \end{bmatrix}
		      = \underbrace{x
			      \begin{bmatrix} a \\ c
			      \end{bmatrix}
			      + y \begin{bmatrix} b \\ d
			      \end{bmatrix}}
		      _{\text{intuition}}
		      = \begin{bmatrix} ax + by \\
			      cx + dy\end{bmatrix} \\
	      \]
\end{itemize}

Note:
\begin{itemize}
	\item by summary: Matrix $\implies$ set of transformed basis vectors
	\item thus giving transformation data of the
	      space(coordinate system)
\end{itemize}


\newpage
\section{Matrix Multiplication as Composition}

\subsection{Composition matrix}
\begin{itemize}
	\item when a series of transformation occurs the
	      "overall" transformation matrix is called
	      as composition matrix
	\item this composition matrix is formed by matrix
	      multiplication of each transformation matrix
	      --from right to left
	\item \[
		      \overset{
		      \substack{
			      f(g(x)) \\
			      \text{Read right to left}
		      }
		      }{
		      \overleftarrow{
		      \underbrace{
			      \begin{bmatrix} 1 & 1 \\
                0 & 1\end{bmatrix}}
		      _{\text{2nd transformation}}
		      \underbrace{
			      \begin{bmatrix} 0 & -1 \\ 1 & 0
			      \end{bmatrix}}
		      _{\text{1st transformation}}
		      }
		      }
		      = \begin{bmatrix} 1 & -1 \\
                1 & 0\end{bmatrix} \\
	      \]
	\item thus we can prove the associative nature of
	      matrix multiplication by just intuition alone
	      \[
		      (AB)C = A(BC)
	      \]
\end{itemize}

\newpage
\section{Three dimensional linear transformations}
Consider a linear transformation with 3d vector as input
and output
\[
	\underset{\text{Input}}{
		\begin{bmatrix} 2 \\ 6 \\ -1 \end{bmatrix}
	}
	\xrightarrow{L (\vec{v})}
	\underset{\text{Output}}{
		\begin{bmatrix} 3 \\ 2 \\ 0 \end{bmatrix}
	}
\]

\begin{itemize}
	\item This means with the given transformation function
	      $L$. The vector point (2, 6, -1) is transformed
	      to (3, 2, 0)
	\item In matrix form $L$ must be a 3 by 3 matrix, since
	      there are 3 basis vectors in 3 dimensions
	\item Ex:
	      \[
		      \begin{bmatrix} 0 & 1 & 2 \\
                3 & 4 & 5 \\
                6 & 7 & 8 \\\end{bmatrix}
	      \]
\end{itemize}

\newpage
\section{Determinant}
\begin{itemize}
	\item To measure the stretching and squishing of a
	      transformation
	\item factor by which the area(in 2D) of the system
	      changes
	\item It is given by "determinant" of a matrix
	      transformation
	\item generally we compare the area change in basis
	      vectors
	\item same way in 3d space volume is scaled and given
	      by determinant
\end{itemize}

\subsection{Computing determinant}
\begin{itemize}
	\item For a $2 \times 2$ matrix the formula is given by
	      \[
		      \text{det}\left(
		      \begin{bmatrix} a & b \\
                c & d \\\end{bmatrix}  \right)
		      = a d - b c
	      \]
	\item The intuition for this formula can be
	      obtained by assuming variables c and d to be 0,
	      then a and d are just the multipliers in
	      their axis for basis vectors in their
	      transformation
	      \[
		      \text{det}\left( \begin{bmatrix}
				      a & 0 \\
				      0 & d \\\end{bmatrix}  \right)
		      = a d - 0 \cdot 0 \\
	      \]
	\item rigorous proof involves finding area of
	      parallelogram
	\item For $3 \times 3$ Matrix:
	      \begin{align*}
		      \text{ det}\left(
		      \begin{bmatrix}
				      a & b & c \\
				      d & e & f \\
				      g & h & i\end{bmatrix}
		      \right)
		       & = a \text{ det}\left(
		      \begin{bmatrix}
				      e & f \\
				      h & i\end{bmatrix}
		      \right)                  \\
		       & - b \text{ det}\left(
		      \begin{bmatrix}
				      d & f \\
				      g & i\end{bmatrix}
		      \right)                  \\
		       & + c \text{ det}\left(
		      \begin{bmatrix}
				      d & e \\
				      g & h\end{bmatrix}
		      \right)                  \\
	      \end{align*}
\end{itemize}

\subsection{Property}
\begin{itemize}
	\item \[
		      \text{det}(M_1 M_2) = \text{det}(M_1) \text{det}(M_2) \\
	      \]
\end{itemize}

\newpage
\section{Inverse Matrices, Rank, Column Space}

\subsection{Usefulness of Matrices}
\begin{itemize}
	\item When we have many linear equations of many variables with degree 1,
	      we can use matrices to solves those equations
	\item  Only things that's happening to those variables are they are
	      scaled by some scalars and added together to get some constant
	      \begin{align*}
		      2x + 5y + 3z & = -3 \\
		      4x + 0y + 8z & = 0  \\
		      1x + 3y + 0z & = 2  \\
	      \end{align*}
	\item this is called as a "linear system of equations", this can be
	      written in matrix form as
	      \begin{align*}
		      \overbrace{
			      \begin{bmatrix}
				      2 & 5 & 3 \\
				      4 & 0 & 8 \\
				      1 & 3 & 0 \\\end{bmatrix}
		      }^{A}
		      \overbrace{
			      \begin{bmatrix}
				      x \\ y \\ z \\ \end{bmatrix}
		      }^{\vec{\text{x}}}
		       & = \overbrace{
			      \begin{bmatrix}
				      -3 \\ 0  \\ 2 \\ \end{bmatrix}
		      }^{\vec{\text{v}}}  \\
		      A \vec{\text{x}}
		       & = \vec{\text{v}} \\
	      \end{align*}
	\item now the problem of three multi variable linear equations
	      has become a single matrix transformation equation
	\item Here exactly, a unknown variable vector $\vec{\text{x}}$
	      after applying a transformation $A$ becomes this exact
	      vector $\vec{\text{v}}$
\end{itemize}

\subsection{Inverse Matrices}
During a transformation, either we can have determinant zero or non-zero,
which implies input dimensions are reduced or not reduced
---lossy transformation ---not sure though

\subsubsection{Non-Zero Determinant}
\begin{itemize}
	\item Thus when we consider a transformation with determinant equals zero
	\item There exists another transformation with the exact opposite/reverse
	      \linebreak movement of the given transformation
	\item thus when both of these transformation are applied in series
	      to a input vector will output the same vector
	      \[
		      \underbrace{A^{-1} A}
		      _{\text{Matrix Multiplication}}
		      = \underbrace{
			      \begin{bmatrix} 1 & 0 \\
                0 & 1 \\\end{bmatrix}
		      }_{\text{The transformation does nothing}}
	      \]
	\item Once we have the inverse matrix $A^{-1}$, we can solve the
	      above equation \newline easily by applying the
	      reverse transformation on the output vector
	      \begin{align*}
		      \underbrace{A^{-1} A }
		      _{\text{The "do nothing" matrix}}
		      \vec{\text{x}}
		       & = A^{-1} \vec{\text{v}} \\
		      \vec{\text{x}}
		       & = A^{-1} \vec{\text{v}} \\
	      \end{align*}
\end{itemize}

\subsubsection{Zero Determinant}
\begin{itemize}
	\item The transformation squishes space into smaller dimension
	\item No inverse transformation(---matrix) exists, since that
	      would require \newline output of multiple vectors for a
	      single input for the inverse transformation
\end{itemize}

\subsection{Rank for a Transformation}
\begin{itemize}
	\item Means no of dimensions in the output of a transformation
	\item Rank 1 means line output, \newline
	      Rank 2 means plane output, \newline
	      Rank 3 means solid output
\end{itemize}

\subsection{Column Space for a Transformation}
\begin{itemize}
	\item Set of all possible output vectors for the given transformation
	      \[
		      A \vec{\text{x}}
	      \]
	\item the name is Column space is used because,
	      columns in a matrix represent the transformed basis vectors
	\item and the span of these transformed basis vectors will give us the
	      Column space of the transformation
\end{itemize}

Note:
\begin{itemize}
	\item Thus the precise definition of rank will be the no of dimensions in
	      the column space
	\item  when the rank is the highest and equal to no of columns,
	      then we call the matrix "full rank"
\end{itemize}

\subsection{Null Space}
\begin{itemize}
	\item Note: In linear transformations the column space will always include
	      origin
	\item for a full-rank transformation, only one vector from the input
	      will land on origin, i.e: the zero vector itself
	\item but for any other lower rank transformations, many input
	      vectors will fall into origin in the output column space
	\item this set of vectors that land on the origin is called the
	      "Null Space"/ "Kernel" of a matrix-transformation
\end{itemize}


\newpage
\section{Non Square Matrices}

\begin{itemize}
	\item For a $3 \times 2$ matrix,
	      \[
		      \begin{bmatrix} 3 & 1 \\
                4 & 1 \\
                5 & 9 \\\end{bmatrix}
	      \]
	\item this takes in 2D input space and outputs a 3D space
	      \[
		      \underbrace{
			      \begin{bmatrix} 2 \\
				      7\end{bmatrix}
		      }_{\text{2d Input}}
		      \to
		      L(\vec{\text{v}})
		      \to
		      \underbrace{
			      \begin{bmatrix} 1 \\
				      8 \\
				      2\end{bmatrix}
		      }_{\text{3d Output}}
	      \]
	\item intuitively this can be observed because we know the columns are the
	      \newline transformed basis vectors, and here the columns are given by
	      3-dimensions and there is only 2 columns which $\implies$ that
	      input space has only 2 basis vectors thus 2 dimensions
	\item also told as "Column space" of this matrix is 3d
\end{itemize}

\subsection{Linearity}
\begin{itemize}
	\item Here Linearity changes for any transformation
	\item Eg: line of evenly spaced dots must remain evenly spaced
\end{itemize}

\newpage
\section{Dot Product and Duality}

\subsection{Dot product standard introduction}
\begin{itemize}
	\item When given two vectors of same dimensions, dot product of them is
	      sum of product of each pair of basis vector scalars
	      \[
		      \underbrace{
			      \begin{bmatrix}
				      2 \\
				      7 \\
				      1\end{bmatrix}
			      \cdot
			      \begin{bmatrix}
				      8 \\
				      2 \\
				      8\end{bmatrix}
		      }_{\text{ Dot product }}
		      = 2 \cdot 8
		      + 7 \cdot 2
		      + 1 \cdot 8
	      \]
	\item It is a "Scalar"
	\item Geometrically the dot product can be given as the product of
	      length of one vector with projected length of second vector
	      on first vector
	\item Note: Mechanical work is the dot product of force and displacement
	      \newline vectors. Magnetic flux is the dot product of
	      the magnetic field and the area vectors.
\end{itemize}

\subsection{$1 \times 2 $ Transformation and Dot product}
\begin{itemize}
	\item Consider the transformation of 2d plane to 1d line,
	      \[
		      \text{Transformation Matrix:}
		      \begin{bmatrix}
			      2 & 1 \end{bmatrix}
	      \]
	\item Then when we apply this transformation to a vector,
	      $ \vec{\text{v}} = \begin{bmatrix} 4 \\ 3 \end{bmatrix} $
	      \[
		      \overbrace{
			      \begin{bmatrix}
				      1 & -2 \end{bmatrix}
		      }^{\text{Transform}}
		      \underbrace{
			      \begin{bmatrix}
				      4 \\ 3 \end{bmatrix}
		      }_{\text{Vector}}
		      = 4 \cdot 1
		      + 3 \cdot -2
	      \]
	\item Now this just resembles dot product of two vectors
	\item This suggests a connection between linear transformation
	      of vectors to numbers ($1 \times 2 \text{ Matrices} $)
	      and Vectors themselves
\end{itemize}

\subsection{Projection as a Transformation}
\begin{itemize}
	\item when we consider a imaginary line on our 2d space with
	      basis vector ($\vec{\text{u}}$)
	\item Projection of any vector(2d) on this line to get
	      a scalar magnitude(1d) can be considered as a
	      "Projection Transformation" with a
	      $1 \times 2$ matrix
	\item By symmetry, Graham finds the transformation
	      matrix for this projection is the matrix of projections
	      of $\vec{\text{u}}$ on x-axis and y-axis
	      \[
		      \text{Projection Transformation: }
		      \begin{bmatrix}
			      u_{x} & u_{y}  \end{bmatrix}
	      \]
	\item Now for any vector in space the projection
	      on the imaginary line can be calculated as
	      \[
		      \overbrace{
			      \begin{bmatrix}
				      u_{x} & u_{y}  \end{bmatrix}
		      }^{\text{Transform}}
		      \underbrace{
			      \begin{bmatrix}
				      x \\
				      y\end{bmatrix}
		      }_{\text{Vector}}
		      = u_{x} \cdot x
		      + u_{y} \cdot y
	      \]
	\item This is exactly similar to dot product of vector
	      with unit vector
	\item This is why "Taking a dot product with a unit vector"
	      can be interpreted the same as
	      "Projection(length) of the vector on the unit vector"
	\item For dot product with non-unit vectors:
	      \begin{itemize}
		      \item we extend our projection method by
		      \item first projection the given vector to the
		            other vector's unit vector
		      \item then scale the projection according to the
		            scale of the other vector
	      \end{itemize}
\end{itemize}

\subsection{Duality}
\begin{itemize}
	\item It loosely means natural but surprising correspondence
	      between two different mathematical thing
	\item Here we can say that the dual of a vector is the
	      linear transformation it encodes
	\item and the dual of a linear transformation of some space to
	      one dimension is a certain vector in the first space
\end{itemize}

\subsection{Conclusion}
\begin{itemize}
	\item Thus dot products are on surface very useful
	      geometric tools for understanding projections
	\item and to test whether two vectors are pointing in
	      similar direction (dot product of perpendiculars is
	      zero)
	\item but on a deeper level we understand that dot
	      product means one vector is matrix transformed onto
	      other vector as number line and scaled by the number line
	      vector subsequently
\end{itemize}

\newpage
\section{Cross Product and Duality}

\subsection{Cross Product Standard Introduction}
\begin{itemize}
	\item In 2d: The parallelogram area enclosed between two
	      vectors is the cross product magnitude of the two
	      vectors
	      \[
		      \vec{\text{v}}
		      \times
		      \vec{\text{w}}
		      = \text{Area of Parallelogram}
	      \]
	\item Trick to remember sign of the cross product: \newline
	      $\hat{i} \times \hat{j}$ is positive, so only if the
	      order of vectors is from right to left
	      the product is positive
	\item They also contain direction, given by "right hand rule",
	      perpendicular to both the vectors/parallelogram
	\item "Notation-Trick" to compute cross product:
	      \begin{align*}
		      \begin{bmatrix}
			      v_1 \\
			      v_2 \\
			      v_3\end{bmatrix}
		      \times
		      \begin{bmatrix}
			      w_1 \\
			      w_2 \\
			      w_3\end{bmatrix}
		       & = \text{det}\left(
		      \begin{bmatrix}
				      \hat{i} & v_1 & w_1 \\
				      \hat{j} & v_2 & w_2 \\
				      \hat{k} & v_3 & w_3\end{bmatrix}
		      \right)                       \\
		       & = \hat{i}(v_2w_3 - v_3w_2)
		      + \hat{j}(v_3w_1 - v_1w_3)
		      + \hat{k}(v_1w_2 - v_2w_1)    \\
	      \end{align*}
\end{itemize}

\subsection{Cross Product as Linear Transformation}

\end{document}
