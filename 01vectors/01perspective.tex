\documentclass[a4paper]{article}

\usepackage{amsmath, amssymb}

\pdfsuppresswarningpagegroup=1

\begin{document}

\section{Vector: 3 perspectives - Physics, Math, CS }

\begin{align*}
	\begin{bmatrix}
		7 & 8 & 6 \\
		6 & 8 & 4 \\
		7 & 5 & 4 \\
	\end{bmatrix}
\end{align*}

\subsection{physics}
\begin{itemize}
	\item vectors are arrows in space with length and direction
	\item so vectors can be moved around in space without any issues
\end{itemize}

\subsection{cs}
\begin{itemize}
	\item vectors are ordered lists of numbers
\end{itemize}

\subsection{math}
\begin{itemize}
	\item seeks to generalize both of these views and defines vector operations such as
	      addition and multiplication
\end{itemize}

\subsection{In linear algebra}
\begin{itemize}
	\item graham suggests to view vector as arrow with tail always fixed at origin
	\item maybe i can imagine a vector as a operation of shifting the origin
	\item thus when we do vector addition we will start the second vector from the
	      start of first vector, since the origin has been shifted by the first vector
	\item Note: "but" it could really be shifting of all the points in the coordinate system
	      (as told by graham)
\end{itemize}

\subsection{Scalars}
\begin{itemize}
	\item The numbers that we multiply the vector with to scale the vector
	      in its original direction
	\item since it's used frequently it just interchangeable with number
\end{itemize}


\newpage
\section{Linear Combinations, Spans, Basis Vectors}

\subsection{Vector Coordinates}
\begin{itemize}
	\item vector coordinates are the number numbers present in a vector
	\item Each of the vector coordinates is also a scalar
	      that scales the basis vectors of the coordinate system $\hat{i}, \hat{j}$
\end{itemize}

\subsection{Span}
Span of $\vec{v} \text{ and } \vec{w}$ are the set of all of their linear combinations
\[
	a \vec{v} + b \vec{w}
\]

Note: Its common to think a collection of vectors as points, due to clustering/noise

\subsection{Linearly dependent Vectors}
\begin{itemize}
	\item If the third vector can be formed by linear combination of the other vectors
	      then the vectors are said to be linearly dependent
	\item If we cannot get a vector by linear combination of other vectors then
	      then those vectors are called as linearly independent
\end{itemize}

\subsection{Basis}
\begin{itemize}
	\item basis of a vector space is a set of linearly independent vectors
	      that span the full space
\end{itemize}

\newpage
\section{Linear Transformation and matrices}
Transformations are just "functions" just suggests to visualize
every function as a movement of coordinates(changing of basis vectors,
thus changing of all of coordinate system)

\subsection{Linear transformation in graph}
\begin{itemize}
	\item every lines will remain parallel
	\item and origin will remain at center
\end{itemize}

\subsection{Matrix Transformation}
\begin{itemize}
	\item A 2 dimensional linear transformation of a coordinates
	      system can be entirely described just 4 numbers (a 2d matrix)
	\item  In a matrix each of the column is the final landing
	      basis vector
	\item thus, matrix $\implies$ transformation data
	\item \[
		      \begin{bmatrix} a & b \\
                c & d\end{bmatrix}
		      \begin{bmatrix} x \\ y
		      \end{bmatrix}
		      = \underbrace{x
			      \begin{bmatrix} a \\ c
			      \end{bmatrix}
			      + y \begin{bmatrix} b \\ d
			      \end{bmatrix}}
		      _{\text{intuition}}
		      = \begin{bmatrix} ax + by \\
			      cx + dy\end{bmatrix} \\
	      \]
\end{itemize}

Note:
\begin{itemize}
	\item by summary: Matrix $\implies$ set of transformed basis vectors
	\item thus giving transformation data of the
	      space(coordinate system)
\end{itemize}


\newpage
\section{Matrix Multiplication as Composition}

\subsection{Composition matrix}
\begin{itemize}
	\item when a series of transformation occurs the
	      "overall" transformation matrix is called
	      as composition matrix
	\item this composition matrix is formed by matrix
	      multiplication of each transformation matrix
	      --from right to left
	\item \[
		      \overset{
		      \substack{
			      f(g(x)) \\
			      \text{Read right to left}
		      }
		      }{
		      \overleftarrow{
		      \underbrace{
			      \begin{bmatrix} 1 & 1 \\
                0 & 1\end{bmatrix}}
		      _{\text{2nd transformation}}
		      \underbrace{
			      \begin{bmatrix} 0 & -1 \\ 1 & 0
			      \end{bmatrix}}
		      _{\text{1st transformation}}
		      }
		      }
		      = \begin{bmatrix} 1 & -1 \\
                1 & 0\end{bmatrix} \\
	      \]
	\item thus we can prove the associative nature of
	      matrix multiplication by just intuition alone
	      \[
		      (AB)C = A(BC)
	      \]
\end{itemize}

\newpage
\section{Three dimensional linear transformations}
Consider a linear transformation with 3d vector as input
and output
\[
	\underset{\text{Input}}{
		\begin{bmatrix} 2 \\ 6 \\ -1 \end{bmatrix}
	}
	\xrightarrow{L (\vec{v})}
	\underset{\text{Output}}{
		\begin{bmatrix} 3 \\ 2 \\ 0 \end{bmatrix}
	}
\]

\begin{itemize}
	\item This means with the given transformation function
	      $L$. The vector point (2, 6, -1) is transformed
	      to (3, 2, 0)
	\item In matrix form $L$ must be a 3 by 3 matrix, since
	      there are 3 basis vectors in 3 dimensions
	\item Ex:
	      \[
		      \begin{bmatrix} 0 & 1 & 2 \\
                3 & 4 & 5 \\
                6 & 7 & 8 \\\end{bmatrix}
	      \]
\end{itemize}

\newpage
\section{Determinant}
\begin{itemize}
	\item To measure the stretching and squishing of a
	      transformation
	\item factor by which the area(in 2D) of the system
	      changes
	\item It is given by "determinant" of a matrix
	      transformation
	\item generally we compare the area change in basis
	      vectors
	\item same way in 3d space volume is scaled and given
	      by determinant
\end{itemize}

\subsection{Computing determinant}
\begin{itemize}
	\item For a $2 \times 2$ matrix the formula is given by
	      \[
		      det\left(
		      \begin{bmatrix} a & b \\
                c & d \\\end{bmatrix}  \right)
		      = a d - b c
	      \]
	\item The intuition for this formula can be
	      obtained by assuming variables c and d to be 0,
	      then a and d are just the multipliers in
	      their axis for basis vectors in their
	      transformation
	      \[
		      det\left( \begin{bmatrix}
				      a & 0 \\
				      0 & d \\\end{bmatrix}  \right)
		      = a d - 0 \cdot 0 \\
	      \]
	\item rigorous proof involves finding area of
	      parallelogram
\end{itemize}

\subsection{Property}
\begin{itemize}
	\item \[
		      det(M_1 M_2) = det(M_1) det(M_2) \\
	      \]
\end{itemize}

\newpage
\section{Inverse Matrices, Rank, Column Space}

\subsection{Usefulness of Matrices}
\begin{itemize}
	\item When we have many linear equations of many variables with degree 1,
	      we can use matrices to solves those equations
	\item  Only things that's happening to those variables are they are
	      scaled by some scalars and added together to get some constant
	      \begin{align*}
		      2x + 5y + 3z & = -3 \\
		      4x + 0y + 8z & = 0  \\
		      1x + 3y + 0z & = 2  \\
	      \end{align*}
	\item this is called as a "linear system of equations", this can be
	      written in matrix form as
	      \begin{align*}
		      \overbrace{
			      \begin{bmatrix}
				      2 & 5 & 3 \\
				      4 & 0 & 8 \\
				      1 & 3 & 0 \\\end{bmatrix}
		      }^{A}
		      \overbrace{
			      \begin{bmatrix}
				      x \\ y \\ z \\ \end{bmatrix}
		      }^{\vec{\text{x}}}
		       & = \overbrace{
			      \begin{bmatrix}
				      -3 \\ 0  \\ 2 \\ \end{bmatrix}
		      }^{\vec{\text{v}}}  \\
		      A \vec{\text{x}}
		       & = \vec{\text{v}} \\
	      \end{align*}
	\item now the problem of three multi variable linear equations
	      has become a single matrix transformation equation
	\item Here exactly, a unknown variable vector $\vec{\text{x}}$
	      after applying a transformation $A$ becomes this exact
	      vector $\vec{\text{v}}$
\end{itemize}

\subsection{Inverse Matrices}
During a transformation, either we can have determinant zero or non-zero,
which implies input dimensions are reduced or not reduced
---lossy transformation ---not sure though

\subsubsection{Non-Zero Determinant}
\begin{itemize}
	\item Thus when we consider a transformation with determinant equals zero
	\item There exists another transformation with the exact opposite/reverse
	      \linebreak movement of the given transformation
	\item thus when both of these transformation are applied in series
	      to a input vector will output the same vector
	      \[
		      \underbrace{A^{-1} A}
		      _{\text{Matrix Multiplication}}
		      = \underbrace{
			      \begin{bmatrix} 1 & 0 \\
                0 & 1 \\\end{bmatrix}
		      }_{\text{The transformation does nothing}}
	      \]
	\item Once we have the inverse matrix $A^{-1}$, we can solve the
	      above equation \newline easily by applying the
	      reverse transformation on the output vector
	      \begin{align*}
		      \underbrace{A^{-1} A }
		      _{\text{The "do nothing" matrix}}
		      \vec{\text{x}}
		       & = A^{-1} \vec{\text{v}} \\
		      \vec{\text{x}}
		       & = A^{-1} \vec{\text{v}} \\
	      \end{align*}
\end{itemize}

\subsubsection{Zero Determinant}
\begin{itemize}
	\item The transformation squishes space into smaller dimension
	\item No inverse transformation(---matrix) exists, since that
	      would require \newline output of multiple vectors for a
	      single input for the inverse transformation
\end{itemize}

\subsection{Rank for a Transformation}
\begin{itemize}
	\item Means no of dimensions in the output of a transformation
	\item Rank 1 means line output, \newline
	      Rank 2 means plane output, \newline
	      Rank 3 means solid output
\end{itemize}

\subsection{Column Space for a Transformation}
\begin{itemize}
	\item Set of all possible output vectors for the given transformation
	      \[
		      A \vec{\text{x}}
	      \]
	\item the name is Column space is used because,
	      columns in a matrix represent the transformed basis vectors
	\item and the span of these transformed basis vectors will give us the
	      Column space of the transformation
\end{itemize}

Note:
\begin{itemize}
	\item Thus the precise definition of rank will be the no of dimensions in
	      the column space
	\item  when the rank is the highest and equal to no of columns,
	      then we call the matrix "full rank"
\end{itemize}

\subsection{Null Space}
\begin{itemize}
	\item Note: In linear transformations the column space will always include
	      origin
	\item for a full-rank transformation, only one vector from the input
	      will land on origin, i.e: the zero vector itself
	\item but for any other lower rank transformations, many input
	      vectors will fall into origin in the output column space
	\item this set of vectors that land on the origin is called the
	      "Null Space"/ "Kernel" of a matrix-transformation
\end{itemize}

\end{document}
